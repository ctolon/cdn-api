version: '3.7'

volumes:

  # Minio
  minio_data:
  minio_cache:

  # RabbitMQ
  rabbitmq-data:
  rabbitmq-logs:
  rabbitmq-plugins:
  rabbitmq-mnesia:

  # Redis
  redis-stack:
  redis-stack-data:

  # Kafka
  kraft-logs:
  kafka-data:
  kafka-config:


networks:
  cdn_net:
    external: true
      

services:

  # go backend
  backend:
    image: cdn-api:latest
    build:
      context: .
      dockerfile: Dockerfile
    container_name: cdn-api
    user: root
    restart: unless-stopped
    networks:
      cdn_net:
        ipv4_address: 10.20.0.2
    ports:
      - "7171:7171"
    stdin_open: true
    tty: true
    platform: linux/amd64
    environment:
      - TZ=Europe/Istanbul
    working_dir: /app
    command: ./app
    volumes:
      - ./.env:/app/.env
    logging:
      driver: "json-file"
      options:
        max-file: "50"
        max-size: "5m"

  # minio nginx
  nginx:
    image: nginx:latest
    container_name: nginx
    restart: unless-stopped
    networks:
      cdn_net:
        ipv4_address: 10.20.0.3
    ports:
      - "9000:9000"
      - "9001:9001"
      - "9002:80"
    volumes:
      - ./infra/minio-nginx.conf:/etc/nginx/nginx.conf
      - minio_data:/data
    depends_on:
      - minio
    logging:
      driver: "json-file"
      options:
        max-file: "50"
        max-size: "5m"

  # minio
  minio:
    image: minio/minio:RELEASE.2024-05-28T17-19-04Z
    container_name: minio
    command: server /data --console-address :9001
    restart: unless-stopped
    networks:
      cdn_net:
        ipv4_address: 10.20.0.4
    #ports:
    #  - "9000:9000"
    #  - "9001:9001"
    expose:
      - "9000"
      - "9001"
    environment:

      # Credentials of the server
      - MINIO_ROOT_USER=${MINIO_ROOT_USER:-minioadmin}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD:-8aBNpGJE6VChP2YkNhsiy7x}

      # Network settings
      #- MINIO_SERVER_URL=http://10.20.0.4:9000

      # Redirect for nginx location /minio/ui
      #- MINIO_BROWSER_REDIRECT_URL=http://localhost:9001/minio/ui

      #- MINIO_BROWSER_REDIRECT_URL=http://10.20.0.4:9000/minio/ui

      # Prometheus settings
      - MINIO_PROMETHEUS_AUTH_TYPE=${MINIO_PROMETHEUS_AUTH_TYPE:-public}

      # Cache settings
      - MINIO_CACHE="on"
      - MINIO_CACHE_DRIVES=/minio_cache
      - MINIO_CACHE_QUOTA=10 # 80% of the drive will be used for caching
      - MINIO_CACHE_AFTER=1 # Object will be cached after the first request to it
      - MINIO_CACHE_WATERMARK_LOW=70 # Least accessed objects will be erased after the cache disk reaches the 70% watermark
      - MINIO_CACHE_WATERMARK_HIGH=90
    volumes:
      - minio_data:/data
      - minio_cache:/minio_cache
    #healthcheck:
      #test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      #interval: 30s
      #timeout: 20s
      #retries: 3
    logging:
      driver: "json-file"
      options:
        max-file: "50"
        max-size: "5m"

  rabbitmq:
    image: rabbitmq:3.12.0-management
    hostname: rabbitmq
    container_name: rabbitmq
    #user: rabbitmq
    restart: unless-stopped
    volumes:
      - ./infra/rabbitmq/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf
      - ./infra/rabbitmq/enabled_plugins:/etc/rabbitmq/enabled_plugins
      - rabbitmq-data:/usr/share/rabbitmq/data/
      - rabbitmq-logs:/var/log/rabbitmq/
      - rabbitmq-plugins:/usr/lib/rabbitmq/plugins/
      - rabbitmq-mnesia:/var/lib/rabbitmq/mnesia/
    environment:
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=admin
      - RABBITMQ_ERLANG_COOKIE=examplecookie
    ports:
      - "5672:5672"
      - "15672:15672"
    networks:
      cdn_net:
        ipv4_address: 10.20.0.5
    healthcheck:
      test: rabbitmq-diagnostics -q ping
      interval: 30s
      timeout: 30s
      retries: 3

  redis:
    image: redis/redis-stack-server:latest
    container_name: redis
    # command: [ "redis-server", "/usr/local/etc/redis/redis.conf" ]
    sysctls:
      - net.core.somaxconn=65535
    restart: unless-stopped
    volumes:
      - redis-stack:/var/lib/redis
      - redis-stack-data:/data
      - ./infra/redis/redis.conf:/etc/redis-stack.conf
    ports:
      - "6379:6379"
    networks:
      cdn_net:
        ipv4_address: 10.20.0.6
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 30s
      retries: 50
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-file: "5"
        max-size: "10m"

  asynqmon:
    image: hibiken/asynqmon:latest
    container_name: asynqmon
    stdin_open: true
    tty: true
    platform: linux/amd64
    restart: on-failure
    environment:
      - TZ=Europe/Istanbul
      - PORT=10001
      - REDIS_ADDR=redis:6379
      - REDIS_PASSWORD=123456
      - REDIS_DB=0
    ports:
      - "10001:10001"
    networks:
      cdn_net:
        ipv4_address: 10.20.0.7
    command: ["./asynqmon", "--redis-addr=redis:6379", "--redis-password=123456", "--redis-db=0", "--port=10001"]
    logging:
      driver: "json-file"
      options:
        max-file: "5"
        max-size: "10m"

  # https://github.com/minhhungit/kafka-kraft-cluster-docker-compose/blob/main/docker-compose.yaml
  kafka:
    image: custom-kafka:latest
    build:
      context: .
      dockerfile: kafka.Dockerfile
      args:
        - CLUSTER_ID=YTY2M2I4Y2Q5NzU5NGY0OD
    container_name: kafka
    hostname: kafka
    restart: unless-stopped
    ports:
      - "39092:39092"
      - "29092:29092" # for access outside docker network
    environment:
      TZ: 'Europe/Istanbul'
      # Node and cluster id metadata
      KAFKA_NODE_ID: 1
      CLUSTER_ID: 'YTY2M2I4Y2Q5NzU5NGY0OD'

      # Thread Settings
      KAFKA_NUM_NETWORK_THREADS: 3
      KAFKA_NUM_IO_THREADS: 8

      # Quorum voters
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka:9093'

      # Log level
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"

      # only good for CDC
      # KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"

      # Listener metadata
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_PROCESS_ROLES: 'broker,controller'

      # Listener configuration for networking
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka:19092,EXTERNAL://kafka:39092,PLAINTEXT_HOST://10.20.0.8:29092,SASL_PLAINTEXT://localhost:49092'
      #KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://kafka:19092
      #KAFKA_LISTENERS: 'PLAINTEXT://kafka:19092,CONTROLLER://kafka:9093,EXTERNAL://kafka:39092,PLAINTEXT_HOST://localhost:29092'
      KAFKA_LISTENERS: 'PLAINTEXT://0.0.0.0:19092,CONTROLLER://0.0.0.0:9093,EXTERNAL://0.0.0.0:39092,PLAINTEXT_HOST://0.0.0.0:29092,SASL_PLAINTEXT://0.0.0.0:49092'
      #KAFKA_LISTENERS: 'PLAINTEXT://kafka:19092,CONTROLLER://kafka:9093,PLAINTEXT_HOST://0.0.0.0:19092'
      KAFKA_ADVERTISED_HOST_NAME: localhost

      #KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka-1:19092,EXTERNAL://10.120.0.5:9092,DOCKER://host.docker.internal:29092 # do not include controller socket here
      #KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT

      KAFKA_LOG_DIRS: '/tmp/kraft-combined-logs'

      # Message and replication configuration
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_MESSAGE_MAX_BYTES: 10000000
      KAFKA_REPLICA_FETCH_MAX_BYTES: 10000000
      # KAFKA_ENABLE_IDEMPOTENCE: "true"

      # Security configuration
      KAFKA_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: PLAIN
      KAFKA_SASL_MECHANISM_CONTROLLER_PROTOCOL: PLAIN
      KAFKA_AUTHORIZER_CLASS_NAME: org.apache.kafka.metadata.authorizer.StandardAuthorizer
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'PLAINTEXT:SASL_PLAINTEXT,CONTROLLER:SASL_PLAINTEXT,EXTERNAL:SASL_PLAINTEXT,PLAINTEXT_HOST:SASL_PLAINTEXT,SASL_PLAINTEXT:SASL_PLAINTEXT'
      #KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "false"
      KAFKA_SUPER_USERS: User:admin
      KAFKA_DELETE_TOPIC_ENABLE: "true"

      # Admin Security
      KAFKA_OPTS: "-Djava.security.auth.login.config=/etc/kafka/kraft/jaas.config"
      #KAFKA_SASL_JAAS_CONFIG: 'org.apache.kafka.common.security.plain.PlainLoginModule required username="admin" password="admin";'
      #KAFKA_SECURITY_PROTOCOL: SASL_PLAINTEXT
      #KAFKA_SASL_MECHANISM: PLAIN
      
      # Socket configuration
      KAFKA_SOCKET_SEND_BUFFER_BYTES: 1048576
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 1048576
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 104857600

      # Retention configuration (persistent for 30 day)
      KAFKA_LOG_RETENTION_HOURS: 720
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_RETENTION_BYTES: -1
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000
      #KAFKA_LOG_RETENTION_MS:


    networks:
      cdn_net:
        ipv4_address: 10.20.0.8
    command:  bash -c "/etc/confluent/docker/run"
    #command: /bin/bash
    #tty: true
    #command: bash -c "/entrypoint/update_run.sh && /etc/confluent/docker/run"
    volumes:
      #- ./entrypoint/update_run.sh:/entrypoint/update_run.sh
      - kraft-logs:/tmp/kraft-combined-logs
      - kafka-data:/var/lib/kafka/data
      - kafka-config:/kafka/config/kraft/server.properties
    logging:
      driver: "json-file"
      options:
        max-file: "5"
        max-size: "10m"

  kafka-web-ui:
    image: gimral/kafka-ui:master
    # image: docker.io/provectuslabs/kafka-ui:521ba0cb2f63110eb2ed13a7054a4d70238a862a
    # image: provectuslabs/kafka-ui:latest
    # image: ghcr.io/kafbat/kafka-ui:latest
    restart: unless-stopped
    container_name: kafka-web-ui
    depends_on:
      - kafka
    ports:
      - '9555:8080'
    networks:
      cdn_net:
        ipv4_address: 10.20.0.9
    environment:

      TZ: 'Europe/Istanbul'
      # Dynamic Kafka Configuration
      DYNAMIC_CONFIG_ENABLED: 'true'

      # Web UI Security
      AUTH_TYPE: 'LOGIN_FORM'
      SPRING_SECURITY_USER_NAME: 'admin'
      SPRING_SECURITY_USER_PASSWORD: 'admin'

      #SPRING_CONFIG_ADDITIONAL-LOCATION: /tmp/config.yml
      # Kafka SASL Security
      KAFKA_CLUSTERS_0_PROPERTIES_SECURITY_PROTOCOL: SASL_PLAINTEXT
      KAFKA_CLUSTERS_0_PROPERTIES_SASL_MECHANISM: PLAIN
      KAFKA_CLUSTERS_0_PROPERTIES_SASL_JAAS_CONFIG: 'org.apache.kafka.common.security.plain.PlainLoginModule required username="admin" password="admin";'

      KAFKA_CLUSTERS_0_NAME: LocalCluster
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:39092